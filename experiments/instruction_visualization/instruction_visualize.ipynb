{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benepar, spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(\"The time for action is now. It's never too late to do something.\")\n",
    "\n",
    "if spacy.__version__.startswith('2'):\n",
    "    nlp.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
    "else:\n",
    "    nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_verb_and_its_dobj(tree_root):\n",
    "    # first check if the current node and its children satisfy the condition\n",
    "    if tree_root.pos_ == \"VERB\":\n",
    "        for child in tree_root.children:\n",
    "            if child.dep_ == \"dobj\" and child.pos_ == \"NOUN\":\n",
    "                return tree_root.lemma_, child.lemma_\n",
    "        return tree_root.lemma_, None\n",
    "    # if not, check its children\n",
    "    for child in tree_root.children:\n",
    "        return find_root_verb_and_its_dobj(child)\n",
    "    # if no children satisfy the condition, return None\n",
    "    return None, None\n",
    "\n",
    "def find_root_verb_and_its_dobj_in_string(s):\n",
    "    doc = nlp(s)\n",
    "    first_sent = list(doc.sents)[0]\n",
    "    return find_root_verb_and_its_dobj(first_sent.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "generated_data_path = \"/mnt/16t/oyx/EasyInstruct/data/evol_instruct_5k.jsonl\"\n",
    "if generated_data_path.endswith(\".jsonl\"):\n",
    "    machine_generated_tasks = [json.loads(l) for l in open(generated_data_path, \"r\")]\n",
    "elif generated_data_path.endswith(\".json\"):\n",
    "    machine_generated_tasks = json.load(open(generated_data_path, \"r\"))\n",
    "else:\n",
    "    raise ValueError(\"Unknown file format\")\n",
    "\n",
    "def process_instruction(instruction):\n",
    "    try:\n",
    "        verb, noun = find_root_verb_and_its_dobj_in_string(instruction)\n",
    "        return {\n",
    "            \"verb\": verb,\n",
    "            \"noun\": noun,\n",
    "            \"instruction\": instruction\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(instruction)\n",
    "        return None\n",
    "    \n",
    "raw_phrases_file = os.path.splitext(os.path.basename(generated_data_path))[0] + \"_raw_phrases.json\"\n",
    "\n",
    "if os.path.exists(raw_phrases_file):\n",
    "    print(\"Raw phrases file already exists, skip processing\")\n",
    "else:\n",
    "    instructions = set([task[\"instruction\"] for task in machine_generated_tasks])\n",
    "    raw_phrases = []\n",
    "\n",
    "    for instruction in tqdm(instructions):\n",
    "        raw_phrases.append(process_instruction(instruction))\n",
    "\n",
    "    with open(raw_phrases_file, \"w\") as f:\n",
    "        json.dump(raw_phrases, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_phrases = json.load(open(raw_phrases_file, \"r\"))\n",
    "raw_phrases = [p for p in raw_phrases if p is not None]\n",
    "raw_phrases = pd.DataFrame(raw_phrases)\n",
    "phrases = pd.DataFrame(raw_phrases).dropna()\n",
    "phrases[[\"verb\", \"noun\"]].groupby([\"verb\", \"noun\"]).size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_verbs = phrases[[\"verb\"]].groupby([\"verb\"]).size().nlargest(20).reset_index()\n",
    "\n",
    "df = phrases[phrases[\"verb\"].isin(top_verbs[\"verb\"].tolist())]\n",
    "# df = df[~df[\"noun\"].isin([\"I\", \"what\"])]\n",
    "# df = phrases\n",
    "# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"verb\"] = \"other\"\n",
    "# df[~df[\"verb\"].isin(top_verbs[\"verb\"].tolist())][\"noun\"] = \"other\"\n",
    "df = df.groupby([\"verb\", \"noun\"]).size().reset_index().rename(columns={0: \"count\"}).sort_values(by=[\"count\"], ascending=False)\n",
    "# df = df[df[\"count\"] > 10]\n",
    "df = df.groupby(\"verb\").apply(lambda x: x.sort_values(\"count\", ascending=False).head(4)).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# df[\"blank\"] = \"ROOT\"\n",
    "# df = phrases.groupby([\"verb\", \"noun\"]).size().sort_values(ascending=False).head(5).reset_index().rename(columns={0: \"count\"})\n",
    "\n",
    "df = df[df[\"count\"] > 5]\n",
    "fig = px.sunburst(df, path=['verb', 'noun'], values='count')\n",
    "# fig.update_layout(uniformtext=dict(minsize=10, mode='hide'))\n",
    "fig.update_layout(\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    "    font_family=\"Times New Roman\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "verb_noun_file = os.path.splitext(os.path.basename(generated_data_path))[0] + \"_verb_noun.html\"\n",
    "fig.write_html(verb_noun_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
